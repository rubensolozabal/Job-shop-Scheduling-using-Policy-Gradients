{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from config.ipynb\n",
      "importing Jupyter notebook from environment.ipynb\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical, Bernoulli\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import time\n",
    "\n",
    "import import_ipynb\n",
    "from config import *\n",
    "from environment import *\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 47\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.machine_profile = \"small_default\"\n",
    "config.job_profile = \"small_default\"\n",
    "config.reconfigure()\n",
    "\n",
    "file_learn = \"datasets/learning_dataset_env_small_default.data\"\n",
    "file_test = \"datasets/testing_dataset_env_small_default_tout1000.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# Read datasets\n",
    "#-----------------------------------------------------------------------------\n",
    "        \n",
    "learning_machines = []\n",
    "learning_duration = []\n",
    "\n",
    "testing_machines = []\n",
    "testing_duration = []\n",
    "testing_solver = []\n",
    "\n",
    "\n",
    "# Learning dataset\n",
    "num_lines = len(open(file_learn).readlines())\n",
    "    \n",
    "with open(file_learn, \"r\") as file:\n",
    "    \n",
    "    for _ in range(num_lines // (config.num_jobs+1)):\n",
    "        \n",
    "        # Read an instance from the learning dataset file\n",
    "        \n",
    "        # First, read the number as jobs and machines on the instance\n",
    "        NB_JOBS, NB_MACHINES = [int(v) for v in file.readline().split()]\n",
    "        \n",
    "        # For all the operations in the job read the machine assigned to the operation and duration time \n",
    "        JOBS = [[int(v) for v in file.readline().split()] for i in range(NB_JOBS)]\n",
    "\n",
    "        # Build list of machines for each operation. MACHINES[j][s] = id of the machine for the operation s of the job j\n",
    "        learning_machines.append([[JOBS[j][2 * s] for s in range(NB_MACHINES)] for j in range(NB_JOBS)])\n",
    "\n",
    "        # Build list of durations for each operation. DURATION[j][s] = duration of the operation s of the job j\n",
    "        learning_duration.append([[JOBS[j][2 * s + 1] for s in range(NB_MACHINES)] for j in range(NB_JOBS)])\n",
    "\n",
    "\n",
    "learning_machines = np.array(learning_machines)\n",
    "learning_duration = np.array(learning_duration)\n",
    "\n",
    "# Testing dataset\n",
    "num_lines = len(open(file_test).readlines())\n",
    "    \n",
    "with open(file_test, \"r\") as file:\n",
    "    \n",
    "    for _ in range(num_lines // (config.num_jobs+2)):\n",
    "        \n",
    "        # Read an instance from the testing dataset file\n",
    "        \n",
    "        # First, read the number as jobs and machines on the instance\n",
    "        NB_JOBS, NB_MACHINES = [int(v) for v in file.readline().split()]\n",
    "        \n",
    "        # For all the operations in the job read the machine assigned to the operation and duration time \n",
    "        JOBS = [[int(v) for v in file.readline().split()] for i in range(NB_JOBS)]\n",
    "        \n",
    "        # Read the solver makespan previously computed\n",
    "        testing_solver.append(int(float(file.readline().split()[1])))\n",
    "        \n",
    "        # Build list of machines. MACHINES[j][s] = id of the machine for the operation s of the job j\n",
    "        testing_machines.append([[JOBS[j][2 * s] for s in range(NB_MACHINES)] for j in range(NB_JOBS)])\n",
    "\n",
    "        # Build list of durations. DURATION[j][s] = duration of the operation s of the job j\n",
    "        testing_duration.append([[JOBS[j][2 * s + 1] for s in range(NB_MACHINES)] for j in range(NB_JOBS)])\n",
    "        \n",
    "\n",
    "testing_machines = np.array(testing_machines)\n",
    "testing_duration = np.array(testing_duration)\n",
    "testing_solver = np.array(testing_solver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTestingError(inference=False):\n",
    "\n",
    "    BSS=20 # Batch size\n",
    "    LSS=40 # Number of times the same instance is repeated into the agent to create the output distribution used to create the self competing baseline\n",
    "\n",
    "\n",
    "    # Placements of the best solution (just to plot the Gantt diagrams)\n",
    "    placementsBest = []\n",
    "    \n",
    "    diffBest = []\n",
    "    averError = []\n",
    "    averDiff = []\n",
    "    \n",
    "    # Number of minibatches\n",
    "    IterPerEpoch = len(testing_machines)//BSS\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for it in range(IterPerEpoch):\n",
    "            \n",
    "            # Select the intances and replicate the instances BSS times\n",
    "            machines = testing_machines[it*BSS:(it+1)*BSS]\n",
    "            machines = np.concatenate([machines for _ in range(LSS)])\n",
    "            duration = testing_duration[it*BSS:(it+1)*BSS]\n",
    "            duration = np.concatenate([duration for _ in range(LSS)])\n",
    "\n",
    "            \n",
    "            # Evaluate into the model\n",
    "            start_time = time.time()\n",
    "            rewards, logProbs, probs, actions_bare, actions, jobTime, placements = actor.forward(machines, duration, inference)\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            # Get the solver solution\n",
    "            cv = testing_solver[it*BSS:(it+1)*BSS]\n",
    "            cv = np.concatenate([cv for j in range(LSS)])\n",
    "            \n",
    "            # Compute the difference between the model and the solver\n",
    "            diff = rewards.cpu().data.numpy() - cv\n",
    "            \n",
    "            # Extract the solution with the lowest gap to the solver from the BSS instances\n",
    "            diffBest.extend(np.min(diff.reshape([LSS,BSS]),0))\n",
    "            \n",
    "            if inference:\n",
    "                \n",
    "                # Compute the placements for the best solution\n",
    "                idxsBest = np.argmin(diff.reshape([LSS,BSS]),0)\n",
    "                placements = placements.cpu().data.numpy()\n",
    "                placements = placements.reshape([LSS,BSS,config.num_machines,config.num_tasksperjob,2])\n",
    "\n",
    "                for i in range(BSS):\n",
    "                    placementsBest.append(placements[idxsBest[i],i,:,:,:])\n",
    "\n",
    "\n",
    "            averDiff.append(np.mean(diff)) # Mean difference to the solver in the BSS instances\n",
    "            averError.append(np.sum(diff > .01)/BSS) # No of samples with error per BSS batch\n",
    "            \n",
    " \n",
    "    return np.mean(averError), np.mean(averDiff), diffBest, placementsBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super(Actor, self).__init__() \n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        #-----------------------------------------------------------------------------\n",
    "        # Nerual Network\n",
    "        #-----------------------------------------------------------------------------\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.jobsEmbedding = nn.Linear(2, config.embedding_size,  bias=False)  \n",
    "        self.stateEmbedding = nn.Linear(config.num_jobs*2, config.embedding_size,  bias=False)  \n",
    "        \n",
    "        # Recurrent encoder used to embed the operations in each job\n",
    "        self.encoder = nn.LSTM(input_size= config.embedding_size, hidden_size=config.embedding_size, num_layers= 1, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # Fully connected layres to output the action distribution\n",
    "        self.activation = F.relu\n",
    "        self.fc1 = nn.Linear(config.embedding_size*(config.num_jobs+1), 128, bias=True)\n",
    "        self.fc2 = nn.Linear(512, 128, bias=True)  # Layer used in: medium, large and xlarge\n",
    "        self.fc3 = nn.Linear(128, config.num_jobs*2, bias=True)  \n",
    "        \n",
    "        #nn.init.xavier_normal(self.fc1.weight)\n",
    "       \n",
    "    \n",
    "    def forward(self, machines, duration, inference=False):\n",
    "        \n",
    "        #-----------------------------------------------------------------------------\n",
    "        # Generate the input tensors\n",
    "        #-----------------------------------------------------------------------------\n",
    "\n",
    "        BS = machines.shape[0]\n",
    "        \n",
    "        machines = np.append(machines, np.zeros((BS, self.config.num_jobs, 1)), axis=2)\n",
    "        duration = np.append(duration, np.zeros((BS, self.config.num_jobs, 1)), axis=2)\n",
    "        \n",
    "\n",
    "        assert self.config.num_jobs == machines.shape[1]\n",
    "        assert self.config.num_tasksperjob +1 == machines.shape[2] \n",
    "\n",
    "        \n",
    "        self.machines = torch.Tensor(machines).to(device)\n",
    "        self.duration = torch.Tensor(duration).to(device)\n",
    "        \n",
    "        \n",
    "        # Create the tensor that defines the instance of the problem\n",
    "        self.jobs = torch.stack((self.machines, self.duration), dim = 3) # [batch, jobs, machines, 2]\n",
    "        \n",
    "        \n",
    "        #-----------------------------------------------------------------------------\n",
    "        # Jobs embedding using the same LSTM\n",
    "        #-----------------------------------------------------------------------------\n",
    "        \n",
    "        # Embedding\n",
    "        embeddedJobs = self.jobsEmbedding(self.jobs) # [batch, jobs, machines, embeddings]\n",
    "        \n",
    "        # Serializing to input into the RNN\n",
    "        serializedJobs = torch.unbind(embeddedJobs, dim=1) # list of [batch, machines, embeddings] for each job\n",
    "     \n",
    "        outputJobs = [[] for _ in range(self.config.num_jobs)]   \n",
    "        \n",
    "        # Compute the recurrent encoding\n",
    "        for i in range(self.config.num_jobs):\n",
    "        \n",
    "            h = torch.zeros([1, BS, config.embedding_size], device=device)\n",
    "            c = torch.zeros([1, BS, config.embedding_size], device=device)\n",
    "\n",
    "            outputs = []\n",
    "            outputsh = []\n",
    "            outputsc = []\n",
    "\n",
    "            for j in range(self.config.num_tasksperjob-1, -1, -1):\n",
    "                \n",
    "                output, (h , c ) = self.encoder(serializedJobs[i][:,j:j+1,:], (h , c ))\n",
    "                outputs.insert(0,output.squeeze(1))\n",
    "                outputsh.insert(0,h.squeeze(0))\n",
    "                outputsc.insert(0,c.squeeze(0))\n",
    "                \n",
    "            outputJobs[i] =  outputsh\n",
    "            \n",
    "        # Add a last task embedded as zeros for every job\n",
    "        emptyEmbeddings = torch.zeros([BS, self.config.embedding_size], device=device)\n",
    "        [job.append(emptyEmbeddings) for job in outputJobs]                                           # [jobs, tasks+1] -> [batch, embedding]\n",
    "        \n",
    "        # Stack the oputput\n",
    "        outputJobs = torch.stack([torch.stack(job, dim=1) for job in outputJobs], dim=1).to(device)   # [batch, jobs, tasks+1, embedding]\n",
    "        \n",
    "        #-----------------------------------------------------------------------------\n",
    "        # Iteration with the environment\n",
    "        #-----------------------------------------------------------------------------\n",
    "        \n",
    "        actions = []\n",
    "        actions_bare = []\n",
    "        probs = []\n",
    "        logProbs = []\n",
    "        \n",
    "        # Index that point to the operations to be scheduled at that time-step\n",
    "        indexes = torch.zeros([BS, self.config.num_jobs], device=device, dtype=torch.long)\n",
    "        \n",
    "        mask = torch.ones([BS, self.config.num_jobs], device=device, dtype=torch.long)\n",
    "        maxTasks = torch.ones([BS, self.config.num_jobs], device=device, dtype=torch.long) * self.config.num_tasksperjob\n",
    "        \n",
    "        # State 0: remaining time for the machine where i should place the task to be free\n",
    "        state0 = torch.zeros([BS, self.config.num_jobs], device=device)\n",
    "        state0T = torch.t(state0)\n",
    "        \n",
    "        # State 1: remaining time for the previous task to end\n",
    "        state1 = torch.zeros([BS, self.config.num_jobs], device=device)\n",
    "        \n",
    "        # Remaining time in each machine\n",
    "        machineTime = torch.zeros([BS, self.config.num_machines], device=device, dtype=torch.float)  # [batch. machines]\n",
    "        \n",
    "        # Remaining time of the prevous task to end = state 1\n",
    "        previousTaskTime = torch.zeros([BS, self.config.num_jobs], device=device, dtype=torch.long)  # [batch, jobs]\n",
    "        \n",
    "        # Makespan is the largest of the job times\n",
    "        jobTime = torch.zeros([BS, self.config.num_jobs], device=device, dtype=torch.long)# [batch, jobs]\n",
    "        \n",
    "        # Action correction variables\n",
    "        tmp = torch.zeros([BS, self.config.num_machines], device=device)\n",
    "        tmp2 = torch.zeros([BS, self.config.num_jobs],  device=device, dtype=torch.long)\n",
    "        tmp2T = torch.t(tmp2)\n",
    "        \n",
    "        # Placements\n",
    "        placements0 = torch.zeros([BS, self.config.num_machines, self.config.num_tasksperjob+1], device=device, dtype=torch.float)\n",
    "        placements1 = torch.zeros([BS, self.config.num_machines, self.config.num_tasksperjob+1], device=device, dtype=torch.float)\n",
    "        placements = torch.stack([placements0[:,:,:-1], placements1[:,:,:-1]], dim=3).int()  \n",
    "        \n",
    "        indx_placements = torch.zeros([BS, self.config.num_machines], device=device, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        #-----------------------------------------------------------------------------\n",
    "        # Allocation loop, while every task has been placed, binary decissions 0/1\n",
    "        #-----------------------------------------------------------------------------\n",
    "\n",
    "        lastOutputJobs = outputJobs[:,:,-1,:]\n",
    "        lastOutputJobs = torch.flatten(lastOutputJobs, start_dim=1)\n",
    "        \n",
    "        while ~torch.all(maxTasks - indexes == 0):   \n",
    "            \n",
    "            indexes_ = torch.unsqueeze(indexes, -1)\n",
    "            indexes_ = torch.unsqueeze(indexes_, -1)\n",
    "            indexes_ = indexes_.expand(-1,-1,-1,self.config.embedding_size)\n",
    "            tasks2place = torch.gather(outputJobs, 2, indexes_).squeeze(2)    # [batch, jobs, embeddings]\n",
    "            tasks2place = torch.flatten(tasks2place, start_dim=1)\n",
    "            \n",
    "            \n",
    "            indexes_ = torch.unsqueeze(indexes, -1)\n",
    "            machinesGather = torch.gather(self.machines, 2, indexes_).squeeze(2).long()       # [batch, job]\n",
    "            durationGather = torch.gather(self.duration, 2, indexes_).squeeze(2).long()       # [batch, job]\n",
    "            \n",
    "            state = torch.stack([state0/10.0, state1/10.0], dim=2)  # Normalize the state\n",
    "            state = torch.flatten(state, start_dim=1)\n",
    "            \n",
    "            # State Embedding\n",
    "            embeddedState = self.stateEmbedding(state) # [batch, jobs, embeddings]\n",
    "    \n",
    "            # Concatenation to create the input to the model\n",
    "            jobsAndStates = torch.cat([tasks2place, embeddedState], 1)    \n",
    "\n",
    "            \n",
    "            # Decoder DNN\n",
    "            out = self.fc1(jobsAndStates)\n",
    "            out = self.fc3(out)\n",
    "            out = out.view(BS,-1, 2)\n",
    "            \n",
    "            # Output distribution\n",
    "            m = Categorical(logits=out)\n",
    "          \n",
    "            \n",
    "            prob = m.probs\n",
    "            entropy = m.entropy()\n",
    "            action_bare = m.sample().long()\n",
    "            log_prob = m.log_prob(action_bare)\n",
    "            \n",
    "\n",
    "            # Mask avoiding to place in machines that are working or place before the previous task ends.\n",
    "            action = action_bare * mask\n",
    "            \n",
    "            ##################################################################\n",
    "            # Action correction: if two tasks that belong to the same machine are \n",
    "            # intended to be placed, only the first one is scheduled.\n",
    "            \n",
    "            tmp[:,:] = 0\n",
    "            tmp2[:,:] = 0\n",
    "            \n",
    "            \n",
    "            for actionT_, machinesGatherT_ in zip(torch.t(action), torch.t(machinesGather)):\n",
    "                tmp.index_put_([torch.tensor(range(BS), device=device), machinesGatherT_], actionT_.float(),  accumulate=True)\n",
    "            \n",
    "            for i, (machinesGatherT_, actionT_) in enumerate(zip(torch.t(machinesGather),torch.t(action))):\n",
    "                \n",
    "                tmp[range(BS), machinesGatherT_] = tmp[range(BS), machinesGatherT_] - actionT_\n",
    "                \n",
    "                w = (tmp[range(BS), machinesGatherT_] == 0) & (actionT_[range(BS)] == 1)\n",
    "                tmp2T[i] = w.float()\n",
    "\n",
    "            \n",
    "            action = tmp2\n",
    "            \n",
    "            ####################################################################\n",
    "            # Update placements\n",
    "            if inference:\n",
    "                for action_, machinesGather_, placements0_, placements1_, indx_placements_, indexes_ in zip(action, machinesGather, placements0, placements1, indx_placements, indexes):\n",
    "\n",
    "                    placements0_.index_put_([machinesGather_, indx_placements_[machinesGather_]], torch.tensor(range(self.config.num_machines),  device=device, dtype=torch.float) * action_.float(),  accumulate=True)\n",
    "                    placements1_.index_put_([machinesGather_, indx_placements_[machinesGather_]], indexes_ * action_.float(),  accumulate=True)\n",
    "\n",
    "                    indx_placements_.put_(machinesGather_, action_,  accumulate=True)\n",
    "\n",
    "                placements = torch.stack([placements0[:,:,:-1], placements1[:,:,:-1]], dim=3).int()   \n",
    "            \n",
    "            \n",
    "            ####################################################################\n",
    "            # Update state\n",
    "            \n",
    "            durationGather = durationGather * action.long()      # [batch, job]\n",
    "            \n",
    "                        \n",
    "            for machinesGatherT_, durationGatherT_  in zip(torch.t(machinesGather), torch.t(durationGather)):\n",
    "                machineTime.index_put_([torch.arange(BS, device=device), machinesGatherT_], durationGatherT_.float(),  accumulate=True)\n",
    "            \n",
    "\n",
    "            previousTaskTime = previousTaskTime + durationGather\n",
    "            \n",
    "            # Update the index of the next tasks to be placed\n",
    "            indexes = indexes + action\n",
    "            \n",
    "            \n",
    "            # Update total time       \n",
    "            jobTime = jobTime + (~(indexes == self.config.num_tasksperjob)).int()\n",
    "            \n",
    "            # Time pases one unit\n",
    "            previousTaskTime = torch.max(previousTaskTime-1 , torch.zeros([BS, self.config.num_jobs], device=device, dtype=torch.long))\n",
    "            machineTime = torch.max(machineTime-1 , torch.zeros([BS, self.config.num_machines], device=device))\n",
    "            \n",
    "            \n",
    "            # Update state\n",
    "            indexes_ = torch.unsqueeze(indexes, -1)\n",
    "            \n",
    "            machinesGather = torch.gather(self.machines, 2, indexes_).squeeze(2).long()               # [batch, job]\n",
    "   \n",
    "            for i, (machinesGatherT_, machineTimeT_)  in enumerate(zip(torch.t(machinesGather), torch.t(machineTime))):\n",
    "                state0T[i] = machineTime[range(BS), machinesGatherT_]\n",
    "\n",
    "\n",
    "            state1 = previousTaskTime.float()\n",
    "            \n",
    "            # Mask to avoid place in machines that are working or place before the previous task ends.\n",
    "            mask = (~(state0.bool() | state1.bool()) & (indexes != self.config.num_tasksperjob)).int()\n",
    "            \n",
    "\n",
    "            # Append\n",
    "            actions.append(action)\n",
    "            actions_bare.append(action_bare)\n",
    "            probs.append(prob)\n",
    "            logProbs.append(log_prob)\n",
    "            \n",
    "\n",
    "        # Stack\n",
    "        logProbs = torch.stack(logProbs)\n",
    "        actions = torch.stack(actions)\n",
    "        actions_bare = torch.stack(actions_bare)\n",
    "        probs = torch.stack(probs)\n",
    "\n",
    "        \n",
    "        # Mask probs after last decision\n",
    "        mask2 = torch.zeros(actions.shape[0], BS, self.config.num_jobs, device=device)\n",
    "        for i in range(BS):\n",
    "            for j in range(self.config.num_jobs):\n",
    "                \n",
    "                mask2[0:jobTime[i,j]+1,i,j] = 1\n",
    "        \n",
    "        logProbs = logProbs * mask2\n",
    "        \n",
    "        \n",
    "        # Adjust jobTime adding last task\n",
    "        jobTime = jobTime + self.duration[:,:,-2].long()\n",
    "        \n",
    "        # Compute the makespan and assign it as the reward\n",
    "        rewards = torch.max(jobTime, dim=1).values\n",
    "        \n",
    "\n",
    "        return rewards, logProbs, probs, actions_bare, actions, jobTime, placements\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the actor\n",
    "actor = Actor(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing the actor...\n",
    "rewards, logProbs, probs, actions_bare, actions, jobTime, placements = actor.forward(testing_machines, testing_duration, inference=True)\n",
    "max_reward = 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning process\n",
    "\n",
    "lr=0.0005 # Learning rate\n",
    "BS=20 # Batch size\n",
    "LS=40 # Number of times the same instance is repeated into the agent to create the output distribution used to create the self competing baseline\n",
    "\n",
    "# No iterations per epoch\n",
    "IterPerEpoch = len(learning_machines)//BS\n",
    "\n",
    "# Instantiate the actor and configure the optimizer\n",
    "actor = Actor(config).to(device)  \n",
    "actor_optim = optim.Adam(actor.parameters(), lr=lr)\n",
    "eps_clip = 0.2\n",
    "\n",
    "# Index of the learning dataset\n",
    "allSamples=[i for i in range(len(learning_machines))]\n",
    "\n",
    "# Decrearning learning rate by gamma every epoch \n",
    "#gamma = np.float_power(1e-1,1/200.0)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(actor_optim, len(allSamples)//BS, gamma=gamma, last_epoch=-1)\n",
    "\n",
    "TESTING_AvgError=[]\n",
    "TESTING_AvgDiff=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run for 50 epochs\n",
    "for i in range(50):\n",
    "\n",
    "    # Shuffle the data in each epoch on the learning dataset\n",
    "    np.random.shuffle(allSamples)\n",
    "        \n",
    "    for it in range(IterPerEpoch):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        actor_optim.zero_grad()\n",
    "\n",
    "        # Select the indexes used on the minibatch\n",
    "        idxs = allSamples[it*BS:(it+1)*BS]\n",
    "\n",
    "        # Extract the instance definition for the minibatch\n",
    "        machines = learning_machines[idxs]\n",
    "        machines = np.concatenate([machines for _ in range(LS)])\n",
    "        duration = learning_duration[idxs]\n",
    "        duration = np.concatenate([duration for _ in range(LS)])\n",
    "\n",
    "        # Run an episode on the problem and return the history\n",
    "        rewards, logProbs, probs, actions_bare, actions, jobTime, _  = actor.forward(machines, duration)\n",
    "        \n",
    "        # Normalize the rewards\n",
    "        rewards = rewards / max_reward\n",
    "\n",
    "        # Aggregate the rewards to reinforce the agent \n",
    "        y = torch.sum(logProbs,0)\n",
    "        logprobs = torch.sum(y,1)\n",
    "\n",
    "        # Reshape the return so that for each instance (ROW), the (COLUMNS) represent the different outputs the stochastic policy produces\n",
    "        RVV = rewards.reshape([LS, BS])\n",
    "        \n",
    "        # Select the baseline as the quantile 'q' of the oputput distribution for each instance\n",
    "        q = np.quantile(RVV.cpu().data.numpy(), 0.1, axis=0)\n",
    "        \n",
    "        # Compute the advantage as the difference between reward and baseline\n",
    "        advantages = RVV-torch.tensor(q).float().to(device)\n",
    "        \n",
    "        # (Optional) Extra computations we were trying to heightened the top advantages the policy produces\n",
    "        ID = (torch.mean(advantages,0) > 0.001)\n",
    "        A = torch.sum(RVV,0)  \n",
    "        B = 1./(0.01+torch.sum(advantages==0,0).float())\n",
    "        advantages+=((advantages<=0)*ID).float()*(-A*B.float())\n",
    "        \n",
    "        # Serialize the advantage\n",
    "        advantages = advantages.view(-1)\n",
    "            \n",
    "        if i == 0:\n",
    "            old_logprobs = logprobs\n",
    "        \n",
    "        # PPO clipping, clip the ratio of change of the new policy\n",
    "        ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - eps_clip, 1 + eps_clip) * advantages\n",
    "        \n",
    "        # Compute the loss as the clipped logprop * advantage\n",
    "        loss = torch.mean(torch.min(surr1, surr2))\n",
    "        \n",
    "        \n",
    "        # Compute backpropagation\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Clip the gradient to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(actor.parameters(), 0.1)\n",
    "        \n",
    "        # Perform a optimization step\n",
    "        actor_optim.step()\n",
    "\n",
    "        end = time.time()\n",
    "        \n",
    "        old_logprobs = logprobs\n",
    "\n",
    "        # Print some learning information\n",
    "        if it%10 == 0:\n",
    "            print(\"------------\", it, \": {0:.2f}s\".format(end - start))\n",
    "            print(\"loss: \", loss.data)  # loss\n",
    "            print(\"RVV: \", torch.mean(rewards.float()).data) # advantage\n",
    "            print(\"logProp: \", torch.mean(logprobs).data) # policy\n",
    "            \n",
    "        \n",
    "        # Plot the tesing error computed during the learning process\n",
    "        if it%10 == 0:\n",
    "            \n",
    "            # Evaluate the testing dataset during the learning process\n",
    "            AvgError, AvgDiff, DiffBest, _ = computeTestingError()\n",
    "            TESTING_AvgError.append(AvgError) # No of samples with error per BSS batch\n",
    "            TESTING_AvgDiff.append(AvgDiff) # Mean difference to the solver in the BSS instances\n",
    "            \n",
    "            # Plot the mean difference between the best solution for the BS trials and the solver solution\n",
    "            print(\"DiffBest: \", np.mean(DiffBest))\n",
    "            plt.semilogy(TESTING_AvgDiff)\n",
    "            plt.grid(True)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ICML2020)",
   "language": "python",
   "name": "icml2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
